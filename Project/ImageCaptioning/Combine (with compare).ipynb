{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10834,"status":"ok","timestamp":1701711911065,"user":{"displayName":"Jennifer Duan","userId":"02371767141346923175"},"user_tz":300},"id":"_6RhwjBzcAYQ","outputId":"8a933f8f-9ff1-4899-fa91-462b3ed8d4fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-12-04 17:45:00--  https://storage.googleapis.com/4995-dlcv-project-data/Flickr8k.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.141.207, 142.251.2.207, 2607:f8b0:4023:c0d::cf\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.141.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1112850179 (1.0G) [application/zip]\n","Saving to: ‘Flickr8k.zip.1’\n","\n","Flickr8k.zip.1      100%[===================>]   1.04G   102MB/s    in 10s     \n","\n","2023-12-04 17:45:10 (104 MB/s) - ‘Flickr8k.zip.1’ saved [1112850179/1112850179]\n","\n"]}],"source":["!wget https://storage.googleapis.com/4995-dlcv-project-data/Flickr8k.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kYcRVRW0mQsz"},"outputs":[],"source":["!mkdir -p \"/content/flickr8k\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5695,"status":"ok","timestamp":1701712117461,"user":{"displayName":"Jennifer Duan","userId":"02371767141346923175"},"user_tz":300},"id":"2BMTE35rngoX","outputId":"47947319-cb20-4c9b-e685-0e770f155332"},"outputs":[{"name":"stdout","output_type":"stream","text":["replace /content/flickr8k/captions.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}],"source":["!unzip -q \"/content/Flickr8k.zip\" -d \"/content/flickr8k\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lSNquW5SpwtP"},"outputs":[],"source":["import os\n","from collections import defaultdict\n","import glob\n","import random\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":149,"status":"ok","timestamp":1701766282928,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"},"user_tz":300},"id":"IV1YIWH3w-YG","outputId":"c9b6b5eb-37be-430b-b91c-414bd7d5275a"},"outputs":[{"output_type":"stream","name":"stdout","text":["count of Flickr8k images :  8091\n"]}],"source":["# Count data\n","flickr8k_data = glob.glob('/content/flickr8k/Images/*.jpg')\n","print(f\"count of Flickr8k images :  {len(flickr8k_data)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":139,"status":"ok","timestamp":1701766284099,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"},"user_tz":300},"id":"VrO9Gcxy3huC","outputId":"098a415e-57cc-47be-ad43-49f91aae2c26"},"outputs":[{"output_type":"stream","name":"stdout","text":["Use cuda device\n"]}],"source":["# Use GPU\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Use {DEVICE} device\")"]},{"cell_type":"markdown","metadata":{"id":"g9gJDqSspg8W"},"source":["### Caption Processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BA94earFp_Y6"},"outputs":[],"source":["# Create a dictionary that has image name as key and all 5 captions as value\n","def read_image_captions(filename):\n","    image_descriptions = defaultdict(list)\n","    with open(filename,'r') as file_list:\n","        next(file_list)\n","        for line in file_list:\n","            line = line.strip()\n","            img_caption_list = line.split(\".jpg,\")\n","            img_name, captions = img_caption_list[0] + \".jpg\", img_caption_list[1]\n","            caption_list = [\"<START>\"] + captions.lower().split(\" \") + [\"<END>\"]\n","            image_descriptions[img_name].append(caption_list)\n","    return image_descriptions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XMOULNDbsaNH"},"outputs":[],"source":["descriptions = read_image_captions(\"/content/flickr8k/captions.txt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":364,"status":"ok","timestamp":1701766288676,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"},"user_tz":300},"id":"wMwRqkYEtoFk","outputId":"cda19a64-a75c-41bf-9361-d504e2f2e62d"},"outputs":[{"output_type":"stream","name":"stdout","text":["[['<START>', 'a', 'black', 'dog', 'and', 'a', 'spotted', 'dog', 'are', 'fighting', '<END>'], ['<START>', 'a', 'black', 'dog', 'and', 'a', 'tri-colored', 'dog', 'playing', 'with', 'each', 'other', 'on', 'the', 'road', '.', '<END>'], ['<START>', 'a', 'black', 'dog', 'and', 'a', 'white', 'dog', 'with', 'brown', 'spots', 'are', 'staring', 'at', 'each', 'other', 'in', 'the', 'street', '.', '<END>'], ['<START>', 'two', 'dogs', 'of', 'different', 'breeds', 'looking', 'at', 'each', 'other', 'on', 'the', 'road', '.', '<END>'], ['<START>', 'two', 'dogs', 'on', 'pavement', 'moving', 'toward', 'each', 'other', '.', '<END>']]\n"]}],"source":["print(descriptions[\"1001773457_577c3a7d70.jpg\"])"]},{"cell_type":"code","source":["# Load the features and IDs\n","loaded_features_list = np.load('/content/features.npy', allow_pickle=True)\n","loaded_ids_list = np.load('/content/ids.npy', allow_pickle=True)\n","# Recreate the dictionary\n","loaded_features_dict = dict(zip(loaded_ids_list, loaded_features_list))"],"metadata":{"id":"112SzPXFNEkV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UfOkRkwpt5ET"},"outputs":[],"source":["# Split the dataset so that train : validation : test is 70 : 15 : 15\n","image_names = list(descriptions.keys())\n","random.shuffle(image_names)\n","total_images = len(image_names)\n","\n","train_end = int(0.7 * total_images)\n","validation_end = train_end + int(0.15 * total_images)\n","\n","train_names = image_names[: train_end]\n","val_names = image_names[train_end : validation_end]\n","test_names = image_names[validation_end :]"]},{"cell_type":"code","source":["# Create a list of image names in the order\n","image_names = list(loaded_features_dict.keys())\n","\n","# Use the image name subsets to create training, validation and test sets\n","train_features = {name: loaded_features_dict[name] for name in train_names}\n","val_features = {name: loaded_features_dict[name] for name in val_names}\n","test_features = {name: loaded_features_dict[name] for name in test_names}"],"metadata":{"id":"qJMDaRd2M7tt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_names[10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"wKIFixaEs8CA","executionInfo":{"status":"ok","timestamp":1701791933689,"user_tz":300,"elapsed":141,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"582b6c7d-1b38-4039-c57b-1a76dfef8fdb"},"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'501650847_b0beba926c.jpg'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["train_features[\"501650847_b0beba926c.jpg\"].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vuxELoOOswIe","executionInfo":{"status":"ok","timestamp":1701791980229,"user_tz":300,"elapsed":144,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"ce530d80-b1cb-45db-e1ce-25d7c516625e"},"execution_count":77,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2048,)"]},"metadata":{},"execution_count":77}]},{"cell_type":"code","source":["import numpy as np\n","\n","# Reshape all features in the dictionary\n","for name in train_features.keys():\n","    feature = train_features[name]\n","    reshaped_feature = np.reshape(feature, (1, 2048))\n","    train_features[name] = reshaped_feature  # Replace original feature with reshaped feature\n","\n","# Now print the shape of a feature to confirm\n","print(train_features[\"501650847_b0beba926c.jpg\"].shape)  # Should output: (1, 2048)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__ke2I-6uH7V","executionInfo":{"status":"ok","timestamp":1701792608449,"user_tz":300,"elapsed":165,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"3ab75e66-f8fe-4267-9631-e20b83c22ce6"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 2048)\n"]}]},{"cell_type":"code","source":["print(train_features[\"501650847_b0beba926c.jpg\"].shape)\n","print(train_features[\"501650847_b0beba926c.jpg\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AEbhe6K6uiuj","executionInfo":{"status":"ok","timestamp":1701792621857,"user_tz":300,"elapsed":141,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"25c468af-b51a-48a9-8b94-a0621514ca19"},"execution_count":83,"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 2048)\n","[[0.35474449 0.32226744 0.46525913 ... 0.24886391 0.00793392 0.16913247]]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6rH4XCIyVyw"},"outputs":[],"source":["# Create mapping for unique words in training data\n","train_tokens = set()\n","for name in train_names:\n","    captions = descriptions[name]\n","    for caption in captions:\n","        for token in caption:\n","            train_tokens.add(token)\n","train_tokens_sorted = sorted(list(train_tokens))\n","\n","id_to_word = {}\n","word_to_id = {}\n","for i, token in enumerate(train_tokens_sorted):\n","    id_to_word[i] = token\n","    word_to_id[token] = i"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155,"status":"ok","timestamp":1701766301546,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"},"user_tz":300},"id":"uCYTkAtMznlD","outputId":"0d98b74e-769f-4d7e-ff8e-0bc6c6ca097f"},"outputs":[{"output_type":"stream","name":"stdout","text":["2093 dog\n"]}],"source":["print(word_to_id[\"dog\"], id_to_word[2093])"]},{"cell_type":"markdown","metadata":{"id":"b1-nEzqS02hO"},"source":["### Decoder Model (LSTM)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":149,"status":"ok","timestamp":1701766304012,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"},"user_tz":300},"id":"SrzaD33X0ihO","outputId":"0d6ecf56-ddc6-4593-d735-c4d8aeea5b7c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum length of a sequence:  37\n"]}],"source":["max_length = max(len(description) for name in train_names for description in descriptions[name])\n","print(\"Maximum length of a sequence: \", max_length)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cuqFiRvezauf"},"outputs":[],"source":["class TextDataset(Dataset):\n","    def __init__(self, train_list, descriptions, word_to_id, max_len, vocab_size):\n","        self.data = []\n","        for img_name in train_list:\n","            captions = descriptions[img_name]\n","            for caption in captions:\n","                for i in range(1, len(caption)):\n","                    encoded_input = [word_to_id[w] for w in caption[:i]]\n","                    # If input sequence is shorter than max_len, pad remaining entries with 0\n","                    if len(encoded_input) < max_len:\n","                        encoded_input += [0] * (max_len - len(encoded_input))\n","                    encoded_output = word_to_id[caption[i]]\n","                    self.data.append((encoded_input, encoded_output))\n","\n","        self.vocab_size = vocab_size\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        input_seq, output_word = self.data[idx]\n","        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(output_word, dtype=torch.long)"]},{"cell_type":"code","source":["class TextDataset(Dataset):\n","    def __init__(self, train_list, descriptions, word_to_id, max_len, vocab_size, image_features):\n","        self.data = []\n","        self.image_features = image_features\n","        for img_name in train_list:\n","            captions = descriptions[img_name]\n","            for caption in captions:\n","                for i in range(1, len(caption)):\n","                    encoded_input = [word_to_id[w] for w in caption[:i]]\n","                    if len(encoded_input) < max_len:\n","                        encoded_input += [0] * (max_len - len(encoded_input))\n","                    encoded_output = word_to_id[caption[i]]\n","\n","                    # Get the corresponding image feature\n","                    img_feature = self.image_features[img_name]\n","\n","                    # Append a tuple of the encoded_input, encoded_output and the image_feature\n","                    self.data.append((encoded_input, encoded_output, img_feature))\n","\n","        self.vocab_size = vocab_size\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        input_seq, output_word, img_feature = self.data[idx]\n","        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(output_word, dtype=torch.long), torch.tensor(img_feature, dtype=torch.float)"],"metadata":{"id":"0Dckzk3JMgkv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6sqbgPy0IuJ"},"outputs":[],"source":["# Create the training dataset and data loader\n","MAX_LEN = max_length\n","vocab_size = len(word_to_id)\n","\n","train_dataset = TextDataset(train_names, descriptions, word_to_id, MAX_LEN, vocab_size)\n","train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)"]},{"cell_type":"code","source":["MAX_LEN = max_length\n","vocab_size = len(word_to_id)\n","\n","train_dataset = TextDataset(train_names, descriptions, word_to_id, MAX_LEN, vocab_size, train_features)\n","train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","#val_dataset = TextDataset(val_names, descriptions, word_to_id, MAX_LEN, vocab_size, val_features)\n","#val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)"],"metadata":{"id":"5HlY01thNPmu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRXSBVTd3gk2"},"outputs":[],"source":["class TextModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n","        super(TextModel, self).__init__()\n","        # Embedding layer: Maps each word to an embedding_dim vector\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        # Bidirectional LSTM\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n","        # Dense layer that outputs the probability distribution over the vocabulary\n","        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, max_len)\n","        # Embedding layer output: (batch_size, max_len, embedding_dim)\n","        x = self.embedding(x)\n","        # LSTM layer output: (batch_size, max_len, hidden_dim * 2)\n","        x, _ = self.lstm(x)\n","        # We only use the output of the last time step\n","        x = x[:, -1, :]\n","        # Fully connected layer output: (batch_size, vocab_size)\n","        x = self.fc(x)\n","\n","        return x"]},{"cell_type":"code","source":["# feature in initial state for LSTM version\n","class ImageCaptioningModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, image_feature_dim):\n","        super(ImageCaptioningModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n","        # Add a layer to transform the image features into a suitable initial state for the LSTM\n","        self.image_fc = nn.Linear(image_feature_dim, hidden_dim)\n","\n","    def forward(self, x, image_features):\n","        x = self.embedding(x)\n","        # Use the transformed image features as the initial hidden state of the LSTM\n","        h0 = self.image_fc(image_features).unsqueeze(0).repeat(2, 1, 1)\n","        c0 = torch.zeros_like(h0)\n","        x, _ = self.lstm(x, (h0, c0))\n","        x = x[:, -1, :]\n","        x = self.fc(x)\n","        return x"],"metadata":{"id":"7mLed2fqNjyU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2u-L29BroBp"},"outputs":[],"source":["# Define the model\n","EMBEDDING_DIM = 300\n","HIDDEN_DIM = 512\n","\n","model = TextModel(vocab_size, EMBEDDING_DIM, HIDDEN_DIM).to(DEVICE)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.RMSprop(model.parameters(), lr=0.005)\n","softmax = nn.Softmax(dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":145,"status":"ok","timestamp":1701712297648,"user":{"displayName":"Jennifer Duan","userId":"02371767141346923175"},"user_tz":300},"id":"fpb5hQJessSz","outputId":"19d06f8d-615d-415a-b251-13dcd1fcab61"},"outputs":[{"data":{"text/plain":["TextModel(\n","  (embedding): Embedding(7763, 300, padding_idx=0)\n","  (lstm): LSTM(300, 512, batch_first=True, bidirectional=True)\n","  (fc): Linear(in_features=1024, out_features=7763, bias=True)\n",")"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6tAplAKXss7d"},"outputs":[],"source":["# Training loop\n","EPOCHS = 2\n","best_train_loss = float('inf')\n","degrade_times = 0\n","threshold = 2\n","for epoch in range(EPOCHS):\n","    train_loss = 0.0\n","    train_corrects = 0\n","    train_count = 0\n","    model.train()\n","    for inputs, targets in train_loader:\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() * len(targets)\n","        pred = softmax(outputs)\n","        train_corrects += (torch.argmax(pred, dim=1) == targets).float().sum()\n","        train_count += targets.size(0)\n","    train_loss = train_loss / len(train_loader.dataset)\n","    train_acc = train_corrects / train_count\n","    print(f\"Epoch {epoch} Train Loss {train_loss:.4f} Train Accuracy {train_acc:.4f}\")\n","\n","    # Check for early stopping\n","    if train_loss < best_train_loss:\n","        best_train_loss = train_loss\n","        degrade_times = 0\n","        torch.save(model.state_dict(), './best_lstm_model.pth')\n","    else:\n","        degrade_times += 1\n","        if degrade_times > threshold:\n","            print(f'Early stopping at epoch {epoch}')\n","            break"]},{"cell_type":"code","source":["image_feature_dim = 2048  # set this to the size of your image features\n","embedding_dim = 200\n","hidden_dim = 300"],"metadata":{"id":"KjtHjq8eNw2U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","model = ImageCaptioningModel(vocab_size, embedding_dim, hidden_dim, image_feature_dim)\n","\n","# Use the GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","num_epochs = 5\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()  # Set model to training mode\n","    running_loss = 0.0\n","    total = 0\n","    correct = 0\n","    for input_seq, output_word, img_feature in train_loader:\n","        # If using GPU, move data to GPU\n","        input_seq = input_seq.to(device)\n","        output_word = output_word.to(device)\n","        img_feature = img_feature.to(device)\n","\n","        # Forward pass\n","        outputs = model(input_seq, img_feature)\n","\n","        # Compute loss\n","        loss = criterion(outputs, output_word)\n","\n","        # Backward pass and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * input_seq.size(0)\n","\n","        # Compute accuracy\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += output_word.size(0)\n","        correct += (predicted == output_word).sum().item()\n","\n","    epoch_loss = running_loss / len(train_dataset)\n","    epoch_acc = correct / total\n","    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}, Accuracy: {epoch_acc}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1071uxQ5Ns6F","executionInfo":{"status":"ok","timestamp":1701767570936,"user_tz":300,"elapsed":287744,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"fa506199-5e50-49a9-c5b6-03d4d381d160"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5, Loss: 4.661154145037747, Accuracy: 0.25505121621248517\n","Epoch 2/5, Loss: 4.054450480040539, Accuracy: 0.3020707363538474\n","Epoch 3/5, Loss: 3.735434069585167, Accuracy: 0.33147795356028603\n","Epoch 4/5, Loss: 3.5170771101098457, Accuracy: 0.35294458709517107\n","Epoch 5/5, Loss: 3.363012059390069, Accuracy: 0.36784560589745713\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":141,"status":"ok","timestamp":1701766590508,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"},"user_tz":300},"id":"2FpcjG3T9WqM","outputId":"77990fc8-dc46-4392-a436-22137af6dd7d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":51}],"source":["# Load the best model\n","model.load_state_dict(torch.load('./best_lstm_model.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ed-srJEjBOd8"},"outputs":[],"source":["# Sample the next word from the distribution returned by the model\n","def sample_decoder():\n","    seq = [\"<START>\"]\n","    while len(seq) < MAX_LEN and seq[-1] != \"<END>\":\n","        encoded_input = [word_to_id[w] for w in seq]\n","        if len(encoded_input) < MAX_LEN:\n","            encoded_input += [0] * (MAX_LEN - len(encoded_input))\n","        encoded_input = torch.tensor(([encoded_input])).to(DEVICE)\n","        outputs = model(encoded_input)\n","        probs = softmax(outputs).cpu()\n","        probs = probs.detach().numpy().astype('float64')\n","        probs = probs[0]\n","        normalized_probs = probs / np.sum(probs)\n","        sampling = np.random.multinomial(1, normalized_probs)\n","        pred_word = id_to_word[np.argmax(sampling)]\n","        seq.append(pred_word)\n","    return seq"]},{"cell_type":"code","source":["def sample_decoder(img_feature, start_seq):\n","    seq = [id_to_word[id.item()] for id in start_seq if id != 0]  # 0 is usually the padding value\n","    while len(seq) < MAX_LEN and seq[-1] != \"<END>\":\n","        encoded_input = [word_to_id[w] for w in seq]\n","        encoded_input = torch.tensor([encoded_input]).to(device)\n","        img_feature = img_feature.to(device)\n","\n","        # Forward pass through the model\n","        with torch.no_grad():\n","            outputs = model(encoded_input, img_feature)\n","\n","        # The output is a distribution over the vocabulary.\n","        # Use the softmax function to convert it to probabilities\n","        probs = F.softmax(outputs, dim=-1)\n","\n","        # Sample a word from the distribution\n","        sampled_word = torch.multinomial(probs[0], 1)\n","\n","        pred_word = id_to_word[sampled_word.item()]\n","        seq.append(pred_word)\n","    return seq\n","\n","# Get an input sequence and the associated image feature from the validation dataset\n","input_seq, _, img_feature = train_dataset[0]\n","\n","# Generate a caption for the image\n","caption = sample_decoder(img_feature, input_seq)\n","\n","# Print the caption\n","print(' '.join(caption))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jJ2l46kDPKol","executionInfo":{"status":"ok","timestamp":1701767599905,"user_tz":300,"elapsed":137,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"cbb18a9e-4804-403f-a072-b358ec42428c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<START> eats the person people rodeo guardrail <END>\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}