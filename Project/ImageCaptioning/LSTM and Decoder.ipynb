{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!wget https://storage.googleapis.com/4995-dlcv-project-data/Flickr8k.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_6RhwjBzcAYQ","executionInfo":{"status":"ok","timestamp":1701711911065,"user_tz":300,"elapsed":10834,"user":{"displayName":"Jennifer Duan","userId":"02371767141346923175"}},"outputId":"8a933f8f-9ff1-4899-fa91-462b3ed8d4fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-12-04 17:45:00--  https://storage.googleapis.com/4995-dlcv-project-data/Flickr8k.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.141.207, 142.251.2.207, 2607:f8b0:4023:c0d::cf\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.141.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1112850179 (1.0G) [application/zip]\n","Saving to: ‘Flickr8k.zip.1’\n","\n","Flickr8k.zip.1      100%[===================>]   1.04G   102MB/s    in 10s     \n","\n","2023-12-04 17:45:10 (104 MB/s) - ‘Flickr8k.zip.1’ saved [1112850179/1112850179]\n","\n"]}]},{"cell_type":"code","source":["!mkdir -p \"/content/flickr8k\""],"metadata":{"id":"kYcRVRW0mQsz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip -q \"/content/Flickr8k.zip\" -d \"/content/flickr8k\""],"metadata":{"id":"2BMTE35rngoX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701712117461,"user_tz":300,"elapsed":5695,"user":{"displayName":"Jennifer Duan","userId":"02371767141346923175"}},"outputId":"47947319-cb20-4c9b-e685-0e770f155332"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["replace /content/flickr8k/captions.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}]},{"cell_type":"code","source":["import os\n","from collections import defaultdict\n","import glob\n","import random\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np"],"metadata":{"id":"lSNquW5SpwtP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count data\n","flickr8k_data = glob.glob('/content/flickr8k/Images/*.jpg')\n","print(f\"count of Flickr8k images :  {len(flickr8k_data)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IV1YIWH3w-YG","executionInfo":{"status":"ok","timestamp":1701712180203,"user_tz":300,"elapsed":141,"user":{"displayName":"Jennifer Duan","userId":"02371767141346923175"}},"outputId":"d17ce953-a92a-4c7f-ef62-aec221bf9efd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["count of Flickr8k images :  8091\n"]}]},{"cell_type":"code","source":["# Use GPU\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Use {DEVICE} device\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VrO9Gcxy3huC","executionInfo":{"status":"ok","timestamp":1701712181746,"user_tz":300,"elapsed":143,"user":{"displayName":"Jennifer Duan","userId":"02371767141346923175"}},"outputId":"d15d97b7-0632-48dc-b43a-2edc080670f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Use cuda device\n"]}]},{"cell_type":"markdown","source":["### Caption Processing"],"metadata":{"id":"g9gJDqSspg8W"}},{"cell_type":"code","source":["# Create a dictionary that has image name as key and all 5 captions as value\n","def read_image_captions(filename):\n","    image_descriptions = defaultdict(list)\n","    with open(filename,'r') as file_list:\n","        next(file_list)\n","        for line in file_list:\n","            line = line.strip()\n","            img_caption_list = line.split(\".jpg,\")\n","            img_name, captions = img_caption_list[0] + \".jpg\", img_caption_list[1]\n","            caption_list = [\"<START>\"] + captions.lower().split(\" \") + [\"<END>\"]\n","            image_descriptions[img_name].append(caption_list)\n","    return image_descriptions"],"metadata":{"id":"BA94earFp_Y6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["descriptions = read_image_captions(\"/content/flickr8k/captions.txt\")"],"metadata":{"id":"XMOULNDbsaNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(descriptions[\"1001773457_577c3a7d70.jpg\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wMwRqkYEtoFk","executionInfo":{"status":"ok","timestamp":1701712189700,"user_tz":300,"elapsed":546,"user":{"displayName":"Jennifer Duan","userId":"02371767141346923175"}},"outputId":"45cfbcf0-affe-460a-edc4-80ffda8e5e15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['<START>', 'a', 'black', 'dog', 'and', 'a', 'spotted', 'dog', 'are', 'fighting', '<END>'], ['<START>', 'a', 'black', 'dog', 'and', 'a', 'tri-colored', 'dog', 'playing', 'with', 'each', 'other', 'on', 'the', 'road', '.', '<END>'], ['<START>', 'a', 'black', 'dog', 'and', 'a', 'white', 'dog', 'with', 'brown', 'spots', 'are', 'staring', 'at', 'each', 'other', 'in', 'the', 'street', '.', '<END>'], ['<START>', 'two', 'dogs', 'of', 'different', 'breeds', 'looking', 'at', 'each', 'other', 'on', 'the', 'road', '.', '<END>'], ['<START>', 'two', 'dogs', 'on', 'pavement', 'moving', 'toward', 'each', 'other', '.', '<END>']]\n"]}]},{"cell_type":"code","source":["# Split the dataset so that train : validation : test is 70 : 15 : 15\n","image_names = list(descriptions.keys())\n","random.shuffle(image_names)\n","total_images = len(image_names)\n","\n","train_end = int(0.7 * total_images)\n","validation_end = train_end + int(0.15 * total_images)\n","\n","train_names = image_names[: train_end]\n","val_names = image_names[train_end : validation_end]\n","test_names = image_names[validation_end :]"],"metadata":{"id":"UfOkRkwpt5ET"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create mapping for unique words in training data\n","train_tokens = set()\n","for name in train_names:\n","    captions = descriptions[name]\n","    for caption in captions:\n","        for token in caption:\n","            train_tokens.add(token)\n","train_tokens_sorted = sorted(list(train_tokens))\n","\n","id_to_word = {}\n","word_to_id = {}\n","for i, token in enumerate(train_tokens_sorted):\n","    id_to_word[i] = token\n","    word_to_id[token] = i"],"metadata":{"id":"j6rH4XCIyVyw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(word_to_id[\"dog\"], id_to_word[2071])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uCYTkAtMznlD","executionInfo":{"status":"ok","timestamp":1701712226281,"user_tz":300,"elapsed":143,"user":{"displayName":"Jennifer Duan","userId":"02371767141346923175"}},"outputId":"0f9bfc69-b08f-4c3f-8763-19b4d121030e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2071 dog\n"]}]},{"cell_type":"markdown","source":["### Decoder Model (LSTM)"],"metadata":{"id":"b1-nEzqS02hO"}},{"cell_type":"code","source":["max_length = max(len(description) for name in train_names for description in descriptions[name])\n","print(\"Maximum length of a sequence: \", max_length)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SrzaD33X0ihO","executionInfo":{"status":"ok","timestamp":1701712232599,"user_tz":300,"elapsed":132,"user":{"displayName":"Jennifer Duan","userId":"02371767141346923175"}},"outputId":"321f8e54-794b-4569-9258-0889be87a636"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum length of a sequence:  37\n"]}]},{"cell_type":"code","source":["class TextDataset(Dataset):\n","    def __init__(self, train_list, descriptions, word_to_id, max_len, vocab_size):\n","        self.data = []\n","        for img_name in train_list:\n","            captions = descriptions[img_name]\n","            for caption in captions:\n","                for i in range(1, len(caption)):\n","                    encoded_input = [word_to_id[w] for w in caption[:i]]\n","                    # If input sequence is shorter than max_len, pad remaining entries with 0\n","                    if len(encoded_input) < max_len:\n","                        encoded_input += [0] * (max_len - len(encoded_input))\n","                    encoded_output = word_to_id[caption[i]]\n","                    self.data.append((encoded_input, encoded_output))\n","\n","        self.vocab_size = vocab_size\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        input_seq, output_word = self.data[idx]\n","        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(output_word, dtype=torch.long)"],"metadata":{"id":"cuqFiRvezauf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the training dataset and data loader\n","MAX_LEN = max_length\n","vocab_size = len(word_to_id)\n","\n","train_dataset = TextDataset(train_names, descriptions, word_to_id, MAX_LEN, vocab_size)\n","train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)"],"metadata":{"id":"j6sqbgPy0IuJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TextModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n","        super(TextModel, self).__init__()\n","        # Embedding layer: Maps each word to an embedding_dim vector\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        # Bidirectional LSTM\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n","        # Dense layer that outputs the probability distribution over the vocabulary\n","        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, max_len)\n","        # Embedding layer output: (batch_size, max_len, embedding_dim)\n","        x = self.embedding(x)\n","        # LSTM layer output: (batch_size, max_len, hidden_dim * 2)\n","        x, _ = self.lstm(x)\n","        # We only use the output of the last time step\n","        x = x[:, -1, :]\n","        # Fully connected layer output: (batch_size, vocab_size)\n","        x = self.fc(x)\n","\n","        return x"],"metadata":{"id":"ZRXSBVTd3gk2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the model\n","EMBEDDING_DIM = 300\n","HIDDEN_DIM = 512\n","\n","model = TextModel(vocab_size, EMBEDDING_DIM, HIDDEN_DIM).to(DEVICE)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.RMSprop(model.parameters(), lr=0.005)\n","softmax = nn.Softmax(dim=1)"],"metadata":{"id":"K2u-L29BroBp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fpb5hQJessSz","executionInfo":{"status":"ok","timestamp":1701712297648,"user_tz":300,"elapsed":145,"user":{"displayName":"Jennifer Duan","userId":"02371767141346923175"}},"outputId":"19d06f8d-615d-415a-b251-13dcd1fcab61"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TextModel(\n","  (embedding): Embedding(7763, 300, padding_idx=0)\n","  (lstm): LSTM(300, 512, batch_first=True, bidirectional=True)\n","  (fc): Linear(in_features=1024, out_features=7763, bias=True)\n",")"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# Training loop\n","EPOCHS = 10\n","best_train_loss = float('inf')\n","degrade_times = 0\n","threshold = 2\n","for epoch in range(EPOCHS):\n","    train_loss = 0.0\n","    train_corrects = 0\n","    train_count = 0\n","    model.train()\n","    for inputs, targets in train_loader:\n","        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item() * len(targets)\n","        pred = softmax(outputs)\n","        train_corrects += (torch.argmax(pred, dim=1) == targets).float().sum()\n","        train_count += targets.size(0)\n","    train_loss = train_loss / len(train_loader.dataset)\n","    train_acc = train_corrects / train_count\n","    print(f\"Epoch {epoch} Train Loss {train_loss:.4f} Train Accuracy {train_acc:.4f}\")\n","\n","    # Check for early stopping\n","    if train_loss < best_train_loss:\n","        best_train_loss = train_loss\n","        degrade_times = 0\n","        torch.save(model.state_dict(), './best_lstm_model.pth')\n","    else:\n","        degrade_times += 1\n","        if degrade_times > threshold:\n","            print(f'Early stopping at epoch {epoch}')\n","            break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6tAplAKXss7d","executionInfo":{"status":"ok","timestamp":1701715596079,"user_tz":300,"elapsed":1009256,"user":{"displayName":"Jennifer Duan","userId":"02371767141346923175"}},"outputId":"a7a556d0-132c-47bb-e733-5964479a69e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 Train Loss 4.0961 Train Accuracy 0.3064\n","Epoch 1 Train Loss 3.4824 Train Accuracy 0.3642\n","Epoch 2 Train Loss 3.3554 Train Accuracy 0.3794\n","Epoch 3 Train Loss 3.2743 Train Accuracy 0.3861\n","Epoch 4 Train Loss 3.2119 Train Accuracy 0.3919\n","Epoch 5 Train Loss 3.1540 Train Accuracy 0.3951\n","Epoch 6 Train Loss 3.1191 Train Accuracy 0.3968\n","Epoch 7 Train Loss 3.0840 Train Accuracy 0.3993\n","Epoch 8 Train Loss 3.0604 Train Accuracy 0.4006\n","Epoch 9 Train Loss 3.0357 Train Accuracy 0.4014\n"]}]},{"cell_type":"code","source":["# Load the best model\n","model.load_state_dict(torch.load('./best_lstm_model.pth'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2FpcjG3T9WqM","executionInfo":{"status":"ok","timestamp":1701715697818,"user_tz":300,"elapsed":162,"user":{"displayName":"Jennifer Duan","userId":"02371767141346923175"}},"outputId":"6e6566f8-4154-4c73-a132-b9067454ba10"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["# Sample the next word from the distribution returned by the model\n","def sample_decoder():\n","    seq = [\"<START>\"]\n","    while len(seq) < MAX_LEN and seq[-1] != \"<END>\":\n","        encoded_input = [word_to_id[w] for w in seq]\n","        if len(encoded_input) < MAX_LEN:\n","            encoded_input += [0] * (MAX_LEN - len(encoded_input))\n","        encoded_input = torch.tensor(([encoded_input])).to(DEVICE)\n","        outputs = model(encoded_input)\n","        probs = softmax(outputs).cpu()\n","        torch.argmax(pred, dim=1)\n","        probs = probs.detach().numpy().astype('float64')\n","        probs = probs[0]\n","        normalized_probs = probs / np.sum(probs)\n","        sampling = np.random.multinomial(1, normalized_probs)\n","        pred_word = id_to_word[np.argmax(sampling)]\n","        seq.append(pred_word)\n","    return seq"],"metadata":{"id":"ed-srJEjBOd8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predicted sequence\n","for i in range(10):\n","    print(sample_decoder())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GK7M_8vCCGUa","executionInfo":{"status":"ok","timestamp":1701721530202,"user_tz":300,"elapsed":478,"user":{"displayName":"Jennifer Duan","userId":"02371767141346923175"}},"outputId":"5d2e4ab3-cd70-423c-e760-d843b24acfeb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['<START>', 'two', 'men', 'throw', 'a', 'young', 'girl', 'in', 'the', 'bathtub', '.', '<END>']\n","['<START>', 'a', 'girl', 'is', 'out', 'for', 'the', 'brown', 'soccer', 'game', '.', '<END>']\n","['<START>', 'an', 'elderly', 'rider', 'on', 'yellow', 'board', '.', '<END>']\n","['<START>', 'one', 'person', 'is', 'holding', 'a', 'in', 'the', 'water', 'in', 'the', 'grass', '.', '<END>']\n","['<START>', 'a', 'white', 'man', 'in', 'a', 'winter', 'vest', 'sitting', 'on', 'a', 'man', 'who', 'is', 'playing', 'tennis', '.', '<END>']\n","['<START>', 'a', 'girl', 'wearing', 'a', 'blue', 'skateboard', 'race', '.', '<END>']\n","['<START>', '\"two', 'men', ',', 'one', 'in', 'costume', ',', ',', 'one', 'has', 'scarf', 'on', 'to', 'the', 'artist', 'in', 'the', 'building', '.\"', '<END>']\n","['<START>', 'a', 'boy', 'is', 'using', 'a', 'fish', 'as', 'he', 'looks', 'at', 'the', 'bottom', '.', '<END>']\n","['<START>', 'two', 'young', 'adults', 'are', 'jumping', 'over', 'rocks', '.', '<END>']\n","['<START>', 'a', 'girl', 'in', 'a', 'pink', 'shirt', 'and', 'a', 'woman', 'are', 'walking', 'through', 'an', 'apple', '.', '<END>']\n"]}]},{"cell_type":"code","source":["import torch\n","\n","array = [1.0, 3.0, 4.0]\n","enc = torch.tensor([array])\n","enc.dtype"],"metadata":{"id":"qWx7Ym-6KRJY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701920758062,"user_tz":300,"elapsed":150,"user":{"displayName":"Jennifer Duan","userId":"02371767141346923175"}},"outputId":"9699f630-76ca-4ab2-d759-057264b30c39"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float32"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":[],"metadata":{"id":"CMWkPLv2YUUi"},"execution_count":null,"outputs":[]}]}