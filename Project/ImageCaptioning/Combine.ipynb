{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPO4klaYXKvLjx2wVrnM+bg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"9QLyHOx-nwfR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701760994843,"user_tz":300,"elapsed":56939,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"92d621e8-f8e2-4fc7-8970-a0192877b9b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-12-05 07:22:17--  https://storage.googleapis.com/4995-dlcv-project-data/Flickr8k.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.10.207, 142.251.12.207, 172.217.194.207, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.10.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1112850179 (1.0G) [application/zip]\n","Saving to: ‘Flickr8k.zip’\n","\n","Flickr8k.zip        100%[===================>]   1.04G  19.9MB/s    in 56s     \n","\n","2023-12-05 07:23:14 (18.9 MB/s) - ‘Flickr8k.zip’ saved [1112850179/1112850179]\n","\n"]}],"source":["!wget https://storage.googleapis.com/4995-dlcv-project-data/Flickr8k.zip"]},{"cell_type":"code","source":["!wget https://storage.googleapis.com/4995-dlcv-project-data/season_images.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IlPlW-5fGWGl","executionInfo":{"status":"ok","timestamp":1701761291732,"user_tz":300,"elapsed":295082,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"d98ac048-4dcd-4671-ffce-7f278f814afd"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-12-05 07:23:16--  https://storage.googleapis.com/4995-dlcv-project-data/season_images.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.12.207, 172.217.194.207, 172.253.118.207, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.12.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5493694674 (5.1G) [application/zip]\n","Saving to: ‘season_images.zip’\n","\n","season_images.zip   100%[===================>]   5.12G  17.7MB/s    in 4m 54s  \n","\n","2023-12-05 07:28:11 (17.8 MB/s) - ‘season_images.zip’ saved [5493694674/5493694674]\n","\n"]}]},{"cell_type":"code","source":["!mkdir -p \"/content/flickr8k\""],"metadata":{"id":"aBewbetNALuk","executionInfo":{"status":"ok","timestamp":1701761341769,"user_tz":300,"elapsed":3,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!mkdir -p \"/content/season_images\""],"metadata":{"id":"-iDJnYT7GmX3","executionInfo":{"status":"ok","timestamp":1701761344482,"user_tz":300,"elapsed":328,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!unzip -q \"/content/Flickr8k.zip\" -d \"/content/flickr8k\""],"metadata":{"id":"ib7DlhC1AUt7","executionInfo":{"status":"ok","timestamp":1701761360349,"user_tz":300,"elapsed":12586,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!unzip -q \"/content/season_images.zip\" -d \"/content/season_images\""],"metadata":{"id":"zAn4zleZGql1","executionInfo":{"status":"ok","timestamp":1701761426425,"user_tz":300,"elapsed":63167,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import os\n","from collections import defaultdict\n","import glob\n","import random\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np"],"metadata":{"id":"iubCaR-XAZjz","executionInfo":{"status":"ok","timestamp":1701761434809,"user_tz":300,"elapsed":3857,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Use {DEVICE} device\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVmkWNfkerTU","executionInfo":{"status":"ok","timestamp":1701761437294,"user_tz":300,"elapsed":501,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"238d729c-4989-41fd-cdb4-3743465bfae7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Use cuda device\n"]}]},{"cell_type":"code","source":["# Count data\n","flickr8k_data = glob.glob('/content/flickr8k/Images/*.jpg')\n","print(f\"count of Flickr8k images :  {len(flickr8k_data)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5SYr4ke5GABd","executionInfo":{"status":"ok","timestamp":1701761440154,"user_tz":300,"elapsed":333,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"73c50d61-bb1b-4e60-ddfc-1be38355e76d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["count of Flickr8k images :  8091\n"]}]},{"cell_type":"code","source":["# Load the features and IDs\n","loaded_features_list = np.load('/content/features.npy', allow_pickle=True)\n","loaded_ids_list = np.load('/content/ids.npy', allow_pickle=True)\n","# Recreate the dictionary\n","loaded_features_dict = dict(zip(loaded_ids_list, loaded_features_list))"],"metadata":{"id":"Jwn4rZymHyUj","executionInfo":{"status":"ok","timestamp":1701761688699,"user_tz":300,"elapsed":2,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["print(len(loaded_features_dict), list(loaded_features_dict.items())[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVSlXBRPK9DA","executionInfo":{"status":"ok","timestamp":1701761692038,"user_tz":300,"elapsed":2,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"eeaae781-66f6-4353-916e-a323eb00e9bc"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["8091 ('102455176_5f8ead62d5.jpg', array([0.36299232, 0.19369601, 0.22639096, ..., 0.15519994, 0.28031123,\n","       0.05308247]))\n"]}]},{"cell_type":"code","source":["# Create a dictionary that has image name as key and all 5 captions as value\n","def read_image_captions(filename):\n","    image_descriptions = defaultdict(list)\n","    with open(filename,'r') as file_list:\n","        next(file_list)\n","        for line in file_list:\n","            line = line.strip()\n","            img_caption_list = line.split(\".jpg,\")\n","            img_name, captions = img_caption_list[0] + \".jpg\", img_caption_list[1]\n","            caption_list = [\"<START>\"] + captions.lower().split(\" \") + [\"<END>\"]\n","            image_descriptions[img_name].append(caption_list)\n","    return image_descriptions"],"metadata":{"id":"d-vE_sFaLAR5","executionInfo":{"status":"ok","timestamp":1701761694659,"user_tz":300,"elapsed":331,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["descriptions = read_image_captions(\"/content/flickr8k/captions.txt\")"],"metadata":{"id":"kU45S9CALGJY","executionInfo":{"status":"ok","timestamp":1701761697096,"user_tz":300,"elapsed":314,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["print(descriptions[\"1001773457_577c3a7d70.jpg\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EYq3dGm4LIuv","executionInfo":{"status":"ok","timestamp":1701761698658,"user_tz":300,"elapsed":2,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"ee8820c4-1c5b-42d0-f6de-1cef2347dc9b"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[['<START>', 'a', 'black', 'dog', 'and', 'a', 'spotted', 'dog', 'are', 'fighting', '<END>'], ['<START>', 'a', 'black', 'dog', 'and', 'a', 'tri-colored', 'dog', 'playing', 'with', 'each', 'other', 'on', 'the', 'road', '.', '<END>'], ['<START>', 'a', 'black', 'dog', 'and', 'a', 'white', 'dog', 'with', 'brown', 'spots', 'are', 'staring', 'at', 'each', 'other', 'in', 'the', 'street', '.', '<END>'], ['<START>', 'two', 'dogs', 'of', 'different', 'breeds', 'looking', 'at', 'each', 'other', 'on', 'the', 'road', '.', '<END>'], ['<START>', 'two', 'dogs', 'on', 'pavement', 'moving', 'toward', 'each', 'other', '.', '<END>']]\n"]}]},{"cell_type":"code","source":["# Split the dataset so that train : validation : test is 70 : 15 : 15\n","image_names = list(descriptions.keys())\n","random.shuffle(image_names)\n","total_images = len(image_names)\n","\n","train_end = int(0.7 * total_images)\n","validation_end = train_end + int(0.15 * total_images)\n","\n","train_names = image_names[: train_end]\n","val_names = image_names[train_end : validation_end]\n","test_names = image_names[validation_end :]"],"metadata":{"id":"r3GkqmO2Rxtr","executionInfo":{"status":"ok","timestamp":1701761700678,"user_tz":300,"elapsed":314,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Create a list of image names in the order\n","image_names = list(loaded_features_dict.keys())\n","\n","# Use the image name subsets to create training, validation and test sets\n","train_features = {name: loaded_features_dict[name] for name in train_names}\n","val_features = {name: loaded_features_dict[name] for name in val_names}\n","test_features = {name: loaded_features_dict[name] for name in test_names}\n"],"metadata":{"id":"pyXfaToBVRQv","executionInfo":{"status":"ok","timestamp":1701766207439,"user_tz":300,"elapsed":324,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}}},"execution_count":100,"outputs":[]},{"cell_type":"code","source":["list(train_features.items())[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AO_FrMQ6pOZy","executionInfo":{"status":"ok","timestamp":1701761705173,"user_tz":300,"elapsed":347,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"2d85831d-9c3e-437b-e0e5-a823799f820a"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('102455176_5f8ead62d5.jpg',\n"," array([0.36299232, 0.19369601, 0.22639096, ..., 0.15519994, 0.28031123,\n","        0.05308247]))"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["max_length = max(len(description) for name in train_names for description in descriptions[name])\n","print(\"Maximum length of a sequence: \", max_length)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tkC-EhAcRfKD","executionInfo":{"status":"ok","timestamp":1701766248645,"user_tz":300,"elapsed":380,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"2d1ad146-6a4e-4c94-d4b2-3bd9244f4645"},"execution_count":102,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum length of a sequence:  40\n"]}]},{"cell_type":"code","source":["# Create mapping for unique words in training data\n","train_tokens = set()\n","for name in train_names:\n","    captions = descriptions[name]\n","    for caption in captions:\n","        for token in caption:\n","            train_tokens.add(token)\n","train_tokens_sorted = sorted(list(train_tokens))\n","\n","id_to_word = {}\n","word_to_id = {}\n","for i, token in enumerate(train_tokens_sorted):\n","    id_to_word[i] = token\n","    word_to_id[token] = i"],"metadata":{"id":"ng2R5da2eaat","executionInfo":{"status":"ok","timestamp":1701761711392,"user_tz":300,"elapsed":349,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["print(word_to_id[\"dog\"], id_to_word[2125])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SIPZr4RW1gvH","executionInfo":{"status":"ok","timestamp":1701761735019,"user_tz":300,"elapsed":2,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"ca821c17-e725-4f0a-a979-22a72a4c0549"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["2125 dog\n"]}]},{"cell_type":"code","source":["class TextDataset(Dataset):\n","    def __init__(self, train_list, descriptions, word_to_id, max_len, vocab_size, image_features):\n","        self.data = []\n","        self.image_features = image_features\n","        for img_name in train_list:\n","            captions = descriptions[img_name]\n","            for caption in captions:\n","                for i in range(1, len(caption)):\n","                    encoded_input = [get_word_id(w, word_to_id) for w in caption[:i]]\n","                    if len(encoded_input) < max_len:\n","                        encoded_input += [0] * (max_len - len(encoded_input))\n","                    encoded_output = get_word_id(caption[i], word_to_id)\n","\n","                    # Get the corresponding image feature\n","                    img_feature = self.image_features[img_name]\n","\n","                    # Append a tuple of the encoded_input, encoded_output and the image_feature\n","                    self.data.append((encoded_input, encoded_output, img_feature))\n","\n","        self.vocab_size = vocab_size\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        input_seq, output_word, img_feature = self.data[idx]\n","        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(output_word, dtype=torch.long), torch.tensor(img_feature, dtype=torch.float)"],"metadata":{"id":"Cz9GzhNQqnhK","executionInfo":{"status":"ok","timestamp":1701761739766,"user_tz":300,"elapsed":1,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["def get_word_id(word, word_to_id):\n","    return word_to_id.get(word, word_to_id[\"<UNK>\"])"],"metadata":{"id":"Ifp_Sc9QlFuN","executionInfo":{"status":"ok","timestamp":1701761742927,"user_tz":300,"elapsed":1,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["MAX_LEN = max_length\n","vocab_size = len(word_to_id)\n","\n","train_dataset = TextDataset(train_names, descriptions, word_to_id, MAX_LEN, vocab_size, train_features)\n","train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n","val_dataset = TextDataset(val_names, descriptions, word_to_id, MAX_LEN, vocab_size, val_features)\n","val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)"],"metadata":{"id":"NfPleteJeOXt","executionInfo":{"status":"ok","timestamp":1701762250435,"user_tz":300,"elapsed":3337,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["val_names[5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"tDS0tShDDAZn","executionInfo":{"status":"ok","timestamp":1701764291349,"user_tz":300,"elapsed":318,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"4798b8cf-1059-462b-8174-12cc3c050092"},"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1569687608_0e3b3ad044.jpg'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["train_names[10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ZW2cTWGLECsw","executionInfo":{"status":"ok","timestamp":1701764426668,"user_tz":300,"elapsed":322,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"72feed7d-d909-4cb2-d170-2bc1985f4087"},"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'3121521593_18f0ec14f7.jpg'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["descriptions[\"3121521593_18f0ec14f7.jpg\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XzhHhJWKCr-x","executionInfo":{"status":"ok","timestamp":1701764436337,"user_tz":300,"elapsed":3,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"8f39e1bc-ed8b-4997-f5a9-0f9832edf1b6"},"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['<START>',\n","  'a',\n","  'smiling',\n","  'girl',\n","  'in',\n","  'glasses',\n","  'and',\n","  'a',\n","  'blue-gray',\n","  'scarf',\n","  'with',\n","  'a',\n","  'smiling',\n","  'girl',\n","  'in',\n","  'a',\n","  'black',\n","  'coat',\n","  'leaning',\n","  'on',\n","  'her',\n","  'shoulder',\n","  '.',\n","  '<END>'],\n"," ['<START>',\n","  'a',\n","  'woman',\n","  'leaning',\n","  'her',\n","  'head',\n","  'on',\n","  'the',\n","  'shoulder',\n","  'of',\n","  'another',\n","  'woman',\n","  '.',\n","  '<END>'],\n"," ['<START>',\n","  '\"two',\n","  'girls',\n","  ',',\n","  'one',\n","  'wearing',\n","  'glasses',\n","  ',',\n","  'dressed',\n","  'in',\n","  'cold',\n","  'weather',\n","  'clothing',\n","  'smile',\n","  'for',\n","  'the',\n","  'camera',\n","  '.\"',\n","  '<END>'],\n"," ['<START>',\n","  'two',\n","  'warmly',\n","  'dress',\n","  'girls',\n","  'posing',\n","  'for',\n","  'a',\n","  'picture',\n","  'outdoors',\n","  '<END>'],\n"," ['<START>',\n","  'two',\n","  'young',\n","  'woman',\n","  'hug',\n","  'and',\n","  'pose',\n","  'together',\n","  'for',\n","  'a',\n","  'picture',\n","  '.',\n","  '<END>']]"]},"metadata":{},"execution_count":71}]},{"cell_type":"code","source":["# features with the word embeddings and feeds them into the LSTM version\n","class ImageCaptioningModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, image_feature_dim):\n","        super(ImageCaptioningModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(embedding_dim + hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n","        # Add a layer to transform the image features\n","        self.image_fc = nn.Linear(image_feature_dim, hidden_dim)\n","\n","    def forward(self, x, image_features):\n","        # Transform the image features\n","        image_features = self.image_fc(image_features).unsqueeze(1)\n","\n","        # Embed the input sequence\n","        x = self.embedding(x)\n","\n","        # Repeat the image features to match the sequence length\n","        image_features = image_features.repeat(1, x.size(1), 1)\n","        x = torch.cat([x, image_features], dim=-1)\n","\n","        # LSTM layer output: (batch_size, max_len, hidden_dim * 2)\n","        x, _ = self.lstm(x)\n","        # We only use the output of the last time step\n","        x = x[:, -1, :]\n","        # Fully connected layer output: (batch_size, vocab_size)\n","        x = self.fc(x)\n","\n","        return x"],"metadata":{"id":"iYa8i1s2QlEZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# feature in initial state for LSTM version\n","class ImageCaptioningModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, image_feature_dim):\n","        super(ImageCaptioningModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n","        # Add a layer to transform the image features into a suitable initial state for the LSTM\n","        self.image_fc = nn.Linear(image_feature_dim, hidden_dim)\n","\n","    def forward(self, x, image_features):\n","        x = self.embedding(x)\n","        # Use the transformed image features as the initial hidden state of the LSTM\n","        h0 = self.image_fc(image_features).unsqueeze(0).repeat(2, 1, 1)\n","        c0 = torch.zeros_like(h0)\n","        x, _ = self.lstm(x, (h0, c0))\n","        x = x[:, -1, :]\n","        x = self.fc(x)\n","        return x"],"metadata":{"id":"0c8ApMbQed-U","executionInfo":{"status":"ok","timestamp":1701765190788,"user_tz":300,"elapsed":333,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["image_feature_dim = 2048  # set this to the size of your image features\n","embedding_dim = 200\n","hidden_dim = 300"],"metadata":{"id":"7-VfgB1Uel3M","executionInfo":{"status":"ok","timestamp":1701762265011,"user_tz":300,"elapsed":2,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z02YXFWsLnT2","executionInfo":{"status":"ok","timestamp":1701766428496,"user_tz":300,"elapsed":318,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"46ae9ccc-3d6d-421f-f9e6-bd21fc51a79d"},"execution_count":103,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ImageCaptioningModel(\n","  (embedding): Embedding(7811, 200, padding_idx=0)\n","  (lstm): LSTM(200, 300, batch_first=True, bidirectional=True)\n","  (fc): Linear(in_features=600, out_features=7811, bias=True)\n","  (image_fc): Linear(in_features=2048, out_features=300, bias=True)\n",")"]},"metadata":{},"execution_count":103}]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","model = ImageCaptioningModel(vocab_size, embedding_dim, hidden_dim, image_feature_dim)\n","\n","# Use the GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","num_epochs = 5\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()  # Set model to training mode\n","    running_loss = 0.0\n","    for input_seq, output_word, img_feature in train_loader:\n","        # If using GPU, move data to GPU\n","        input_seq = input_seq.to(device)\n","        output_word = output_word.to(device)\n","        img_feature = img_feature.to(device)\n","\n","        # Forward pass\n","        outputs = model(input_seq, img_feature)\n","\n","        # Compute loss\n","        loss = criterion(outputs, output_word)\n","\n","        # Backward pass and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * input_seq.size(0)\n","\n","    epoch_loss = running_loss / len(train_dataset)\n","    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}')\n","\n","    # Validation loop\n","    model.eval()  # Set model to evaluation mode\n","    running_val_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for val_input_seq, val_output_word, val_img_feature in val_loader:\n","            # If using GPU, move data to GPU\n","            val_input_seq = val_input_seq.to(device)\n","            val_output_word = val_output_word.to(device)\n","            val_img_feature = val_img_feature.to(device)\n","\n","            # Forward pass\n","            val_outputs = model(val_input_seq, val_img_feature)\n","\n","            # Compute loss\n","            val_loss = criterion(val_outputs, val_output_word)\n","            running_val_loss += val_loss.item() * val_input_seq.size(0)\n","\n","            # Compute accuracy\n","            _, predicted = torch.max(val_outputs.data, 1)\n","            total += val_output_word.size(0)\n","            correct += (predicted == val_output_word).sum().item()\n","\n","    epoch_val_loss = running_val_loss / len(val_dataset)\n","    accuracy = correct / total * 100  # Compute the accuracy\n","    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {epoch_val_loss}, Accuracy: {accuracy}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HbHQtOEu_u6a","executionInfo":{"status":"ok","timestamp":1701765534208,"user_tz":300,"elapsed":341179,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"bb9a3e88-0e21-4379-cfce-faf77a3bac0e"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5, Loss: 4.664137127982076\n","Epoch 1/5, Validation Loss: 4.392096644088375, Accuracy: 28.239490314159806\n","Epoch 2/5, Loss: 4.180260459329399\n","Epoch 2/5, Validation Loss: 4.171144671843194, Accuracy: 29.732104780243212\n","Epoch 3/5, Loss: 4.053153982923867\n","Epoch 3/5, Validation Loss: 4.178437808234081, Accuracy: 29.75536630438997\n","Epoch 4/5, Loss: 4.131807929132734\n","Epoch 4/5, Validation Loss: 4.210902051707148, Accuracy: 29.5899510215686\n","Epoch 5/5, Loss: 4.026902520270426\n","Epoch 5/5, Validation Loss: 4.240431427658302, Accuracy: 29.295305049043048\n"]}]},{"cell_type":"code","source":["def sample_decoder(img_feature):\n","    seq = [\"<START>\"]\n","    while len(seq) < MAX_LEN and seq[-1] != \"<END>\":\n","        encoded_input = [word_to_id[w] for w in seq]\n","        encoded_input = torch.tensor([encoded_input]).to(device)\n","        if len(encoded_input[0]) < MAX_LEN:\n","            padding = torch.zeros((1, MAX_LEN - len(encoded_input[0]))).long().to(device)\n","            encoded_input = torch.cat((encoded_input, padding), dim=1)\n","\n","        # Forward pass through the model\n","        with torch.no_grad():\n","            outputs = model(encoded_input, img_feature)\n","\n","        # The output is a distribution over the vocabulary.\n","        # Use the softmax function to convert it to probabilities\n","        probs = F.softmax(outputs, dim=-1)\n","\n","        # Sample a word from the distribution\n","        sampled_word = torch.multinomial(probs[0], 1)\n","\n","        pred_word = id_to_word[sampled_word.item()]\n","        seq.append(pred_word)\n","    return seq"],"metadata":{"id":"G6EoKxx7AmZS","executionInfo":{"status":"ok","timestamp":1701765568692,"user_tz":300,"elapsed":398,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["def sample_decoder(img_feature, start_seq):\n","    seq = [id_to_word[id.item()] for id in start_seq if id != 0]  # 0 is usually the padding value\n","    while len(seq) < MAX_LEN and seq[-1] != \"<END>\":\n","        encoded_input = [word_to_id[w] for w in seq]\n","        encoded_input = torch.tensor([encoded_input]).to(device)\n","        img_feature = img_feature.to(device)\n","\n","        # Forward pass through the model\n","        with torch.no_grad():\n","            outputs = model(encoded_input, img_feature)\n","\n","        # The output is a distribution over the vocabulary.\n","        # Use the softmax function to convert it to probabilities\n","        probs = F.softmax(outputs, dim=-1)\n","\n","        # Sample a word from the distribution\n","        sampled_word = torch.multinomial(probs[0], 1)\n","\n","        pred_word = id_to_word[sampled_word.item()]\n","        seq.append(pred_word)\n","    return seq\n","\n","# Get an input sequence and the associated image feature from the validation dataset\n","input_seq, _, img_feature = val_dataset[0]\n","\n","# Generate a caption for the image\n","caption = sample_decoder(img_feature, input_seq)\n","\n","# Print the caption\n","print(' '.join(caption))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O8hJn40XFD2d","executionInfo":{"status":"ok","timestamp":1701767683131,"user_tz":300,"elapsed":371,"user":{"displayName":"Shiying Chen","userId":"06264104105624227651"}},"outputId":"96e85585-8c6d-46fa-83d1-b065652bec8e"},"execution_count":118,"outputs":[{"output_type":"stream","name":"stdout","text":["<START> sled white kitchen others on bat <END>\n"]}]}]}